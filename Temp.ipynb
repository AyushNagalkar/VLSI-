{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676318f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch first (CUDA 11.8)\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
    "\n",
    "# Install PyTorch Geometric and extensions using pre-built wheels\n",
    "!pip install torch-geometric -q\n",
    "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.7.0+cu118.html -q\n",
    "\n",
    "# Install other utilities\n",
    "!pip install matplotlib numpy scipy pandas psutil -q\n",
    "\n",
    "print(\"All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ab990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv, GCNConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Numerical and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# System utilities\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import psutil\n",
    "from scipy.spatial import KDTree\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\nAll libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c3c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURE YOUR CIRCUITNET PATHS HERE\n",
    "# ============================================\n",
    "\n",
    "CIRCUITNET_BASE = r\"H:\\Labs\\Generative Ai\\Ayush1\\Ayush\\CircuitNet\"\n",
    "\n",
    "# Data paths\n",
    "PLACEMENT_PATH = os.path.join(CIRCUITNET_BASE, \"instance_placement_micron-002\", \"instance_placement_micron\")\n",
    "NODE_ATTR_PATH = os.path.join(CIRCUITNET_BASE, \"graph_features\", \"graph_information\", \"node_attr\")\n",
    "NET_ATTR_PATH = os.path.join(CIRCUITNET_BASE, \"graph_features\", \"graph_information\", \"net_attr\")\n",
    "PIN_ATTR_PATH = os.path.join(CIRCUITNET_BASE, \"graph_features\", \"graph_information\", \"pin_attr\")\n",
    "CONGESTION_PATH = os.path.join(CIRCUITNET_BASE, \"congestion\")\n",
    "\n",
    "# Verify paths exist\n",
    "print(\"Verifying dataset paths...\\n\")\n",
    "\n",
    "paths_to_check = {\n",
    "    \"Base Directory\": CIRCUITNET_BASE,\n",
    "    \"Placement Data\": PLACEMENT_PATH,\n",
    "    \"Node Attributes\": NODE_ATTR_PATH,\n",
    "    \"Net Attributes\": NET_ATTR_PATH,\n",
    "    \"Pin Attributes\": PIN_ATTR_PATH,\n",
    "}\n",
    "\n",
    "all_exist = True\n",
    "for name, path in paths_to_check.items():\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"[OK]\" if exists else \"[MISSING]\"\n",
    "    print(f\"{status} {name}: {path}\")\n",
    "    if not exists:\n",
    "        all_exist = False\n",
    "\n",
    "print()\n",
    "if all_exist:\n",
    "    print(\"All paths verified! Ready to load data.\")\n",
    "else:\n",
    "    print(\"WARNING: Some paths are missing. Please check your CircuitNet installation.\")\n",
    "    print(\"\\nDownload from: https://drive.google.com/drive/folders/1GjW-1LBx1563bg3pHQGvhcEyK2A9sYUB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "MAX_SAMPLES = 500  # Increased from 100 for better generalization\n",
    "\n",
    "print(f\"Loading {MAX_SAMPLES} samples from CircuitNet...\\n\")\n",
    "circuitnet_dataset = load_circuitnet_dataset(max_samples=MAX_SAMPLES)\n",
    "\n",
    "if circuitnet_dataset:\n",
    "    # Split into train/test (80/20)\n",
    "    split_idx = int(len(circuitnet_dataset) * 0.8)\n",
    "    cn_train = circuitnet_dataset[:split_idx]\n",
    "    cn_test = circuitnet_dataset[split_idx:]\n",
    "    \n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"   Total samples: {len(circuitnet_dataset)}\")\n",
    "    print(f\"   Training samples: {len(cn_train)}\")\n",
    "    print(f\"   Test samples: {len(cn_test)}\")\n",
    "    print(f\"   Cells per sample: ~{circuitnet_dataset[0].num_cells:,}\")\n",
    "    print(f\"   Edges per sample: ~{circuitnet_dataset[0].edge_index.shape[1]:,}\")\n",
    "    print(f\"\\nDataset ready for training!\")\n",
    "else:\n",
    "    print(\"Failed to load dataset. Check your paths!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c2def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLSIPlacementGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention Network for VLSI cell placement prediction\n",
    "    \n",
    "    Industry-Aware Architecture (v2 - Anti-Collapse):\n",
    "    - Input: 16 features per cell (size, connectivity, macro classification, context)\n",
    "    - 4 GAT layers with multi-head attention + residual connections\n",
    "    - NO Sigmoid bottleneck - uses scaled tanh for full [0,1] range utilization\n",
    "    - Residual connections prevent gradient vanishing in deep GNN\n",
    "    - Output: (x, y) coordinates for each cell\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=16, hidden_dim=128, output_dim=2, num_layers=4, heads=4):\n",
    "        super(VLSIPlacementGNN, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.input_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # GAT layers with attention + layer norms for residual connections\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.layer_norms = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = hidden_dim\n",
    "            out_channels = hidden_dim\n",
    "            self.gat_layers.append(\n",
    "                GATConv(in_channels, out_channels // heads, heads=heads, dropout=0.1, concat=True)\n",
    "            )\n",
    "            self.layer_norms.append(nn.LayerNorm(hidden_dim))\n",
    "        \n",
    "        # Output projection - no Sigmoid! Use clamped output instead\n",
    "        # Sigmoid causes center collapse by squashing gradients at extremes\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # Input projection\n",
    "        x = self.input_proj(x)\n",
    "        x = self.input_norm(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # GAT layers with RESIDUAL connections\n",
    "        for i, (gat_layer, layer_norm) in enumerate(zip(self.gat_layers, self.layer_norms)):\n",
    "            residual = x\n",
    "            x = gat_layer(x, edge_index)\n",
    "            x = layer_norm(x)\n",
    "            if i < len(self.gat_layers) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.1, training=self.training)\n",
    "            # Residual connection - prevents information loss in deep GNN\n",
    "            x = x + residual\n",
    "        \n",
    "        # Output projection\n",
    "        out = self.output_proj(x)\n",
    "        \n",
    "        # Clamp to [0, 1] instead of Sigmoid\n",
    "        # This preserves gradients at boundaries (Sigmoid kills them)\n",
    "        out = out.clamp(0.0, 1.0)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Create model with 16 input features (industry-relevant)\n",
    "model = VLSIPlacementGNN(\n",
    "    input_dim=16,      # 16 industry-relevant features\n",
    "    hidden_dim=128,\n",
    "    output_dim=2,\n",
    "    num_layers=4,\n",
    "    heads=4\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model Architecture (v2 - Anti-Collapse):\\n\")\n",
    "print(model)\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"   Total parameters: {num_params:,}\")\n",
    "print(f\"   Input features: 16 dimensions (industry-relevant)\")\n",
    "print(f\"   Key improvements over v1:\")\n",
    "print(f\"     - Residual connections (prevents gradient vanishing)\")\n",
    "print(f\"     - LayerNorm (stabilizes training)\")\n",
    "print(f\"     - No Sigmoid (prevents center collapse)\")\n",
    "print(f\"     - Clamp [0,1] output (preserves gradients at boundaries)\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(\"\\nModel created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede207b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with resume capability\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "start_epoch = 0\n",
    "\n",
    "# Check if model exists and load it\n",
    "model_save_path = r\"H:\\Labs\\Generative Ai\\Ayush\\vlsi_placement_model.pth\"\n",
    "\n",
    "if os.path.exists(model_save_path):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"LOADING EXISTING MODEL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    checkpoint = torch.load(model_save_path)\n",
    "    \n",
    "    # Check if saved model has same architecture (input_dim may differ)\n",
    "    saved_config = checkpoint.get('model_config', {})\n",
    "    saved_input_dim = saved_config.get('input_dim', 10)\n",
    "    current_input_dim = model.input_dim\n",
    "    \n",
    "    if saved_input_dim != current_input_dim:\n",
    "        print(f\"   WARNING: Saved model has input_dim={saved_input_dim}, current model has input_dim={current_input_dim}\")\n",
    "        print(f\"   Architecture changed (new industry-relevant features). Training from scratch.\")\n",
    "        print(\"=\" * 80)\n",
    "        print()\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        train_losses = checkpoint.get('train_losses', [])\n",
    "        test_losses = checkpoint.get('test_losses', [])\n",
    "        \n",
    "        print(f\"Model loaded from: {model_save_path}\")\n",
    "        print(f\"   Previous epochs completed: {start_epoch}\")\n",
    "        print(f\"   Previous train loss: {train_losses[-1]:.6f}\" if train_losses else \"   No previous train loss\")\n",
    "        print(f\"   Previous test loss: {test_losses[-1]:.6f}\" if test_losses else \"   No previous test loss\")\n",
    "        print(f\"   Resuming training from epoch {start_epoch + 1}\")\n",
    "        print(\"=\" * 80)\n",
    "        print()\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STARTING TRAINING FROM SCRATCH\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "print(\"Starting Training...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    # Training\n",
    "    for data in cn_train:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(data)\n",
    "        loss = criterion(pred, data.y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        batch_count += 1\n",
    "    \n",
    "    avg_train_loss = epoch_loss / batch_count\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in cn_test:\n",
    "            data = data.to(device)\n",
    "            pred = model(data)\n",
    "            loss = criterion(pred, data.y)\n",
    "            test_loss += loss.item()\n",
    "            test_count += 1\n",
    "    \n",
    "    avg_test_loss = test_loss / test_count\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1:2d}/{start_epoch + NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.6f} | \"\n",
    "          f\"Test Loss: {avg_test_loss:.6f} | \"\n",
    "          f\"Time: {elapsed:.1f}s\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTraining complete! Total time: {elapsed/60:.1f} minutes\")\n",
    "print(f\"   Final train loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"   Final test loss: {test_losses[-1]:.6f}\")\n",
    "print(f\"   Total epochs completed: {start_epoch + NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_save_path = r\"H:\\Labs\\Generative Ai\\Ayush1\\Ayush\\vlsi_placement_model.pth\"\n",
    "\n",
    "# Calculate total epochs (start_epoch + NUM_EPOCHS)\n",
    "total_epochs = start_epoch + NUM_EPOCHS\n",
    "\n",
    "torch.save({\n",
    "    'epoch': total_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'test_losses': test_losses,\n",
    "    'model_config': {\n",
    "        'input_dim': 16,\n",
    "        'hidden_dim': 128,\n",
    "        'output_dim': 2,\n",
    "        'num_layers': 4,\n",
    "        'heads': 4\n",
    "    }\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "print(f\"   Total epochs completed: {total_epochs}\")\n",
    "print(f\"   File size: {os.path.getsize(model_save_path) / 1e6:.2f} MB\")\n",
    "print(f\"\\nTo load later:\")\n",
    "print(f\"   checkpoint = torch.load('{model_save_path}')\")\n",
    "print(f\"   model.load_state_dict(checkpoint['model_state_dict'])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f065c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_industry_layout(data, predictions, chip_width_microns=1000, chip_height_microns=1000, dpi=150):\n",
    "    \"\"\"\n",
    "    Industry-grade layout visualization with micron precision\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8), dpi=dpi)\n",
    "    \n",
    "    # Convert normalized coordinates to microns\n",
    "    predicted_microns = predictions * np.array([chip_width_microns, chip_height_microns])\n",
    "    actual_microns = data.y.cpu().numpy() * np.array([chip_width_microns, chip_height_microns])\n",
    "    \n",
    "    # Plot 1: Predicted Layout\n",
    "    ax1 = axes[0]\n",
    "    ax1.set_xlim(0, chip_width_microns)\n",
    "    ax1.set_ylim(0, chip_height_microns)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.set_facecolor('#1a1a1a')\n",
    "    \n",
    "    # Draw cells with different colors based on size\n",
    "    for i, (x, y) in enumerate(predicted_microns):\n",
    "        # Estimate cell size from node features (simplified)\n",
    "        cell_width = max(5, min(50, data.x[i, 3].item() * 20)) if data.x.shape[1] > 3 else 10\n",
    "        cell_height = cell_width * 0.8\n",
    "        \n",
    "        rect = Rectangle((x - cell_width/2, y - cell_height/2), \n",
    "                        cell_width, cell_height,\n",
    "                        facecolor='cyan', edgecolor='white', \n",
    "                        alpha=0.7, linewidth=0.5)\n",
    "        ax1.add_patch(rect)\n",
    "    \n",
    "    # Draw connections (sample)\n",
    "    edge_index = data.edge_index.cpu().numpy()\n",
    "    for i in range(0, min(500, edge_index.shape[1]), 5):  # Sample edges\n",
    "        src, dst = edge_index[:, i]\n",
    "        ax1.plot([predicted_microns[src, 0], predicted_microns[dst, 0]],\n",
    "                [predicted_microns[src, 1], predicted_microns[dst, 1]],\n",
    "                'yellow', alpha=0.2, linewidth=0.3)\n",
    "    \n",
    "    ax1.set_title('Predicted Layout (Micron Precision)', fontsize=14, color='white', pad=20)\n",
    "    ax1.set_xlabel('X Position (µm)', fontsize=12, color='white')\n",
    "    ax1.set_ylabel('Y Position (µm)', fontsize=12, color='white')\n",
    "    ax1.tick_params(colors='white')\n",
    "    ax1.grid(True, alpha=0.2, color='gray')\n",
    "    \n",
    "    # Plot 2: Actual Layout\n",
    "    ax2 = axes[1]\n",
    "    ax2.set_xlim(0, chip_width_microns)\n",
    "    ax2.set_ylim(0, chip_height_microns)\n",
    "    ax2.set_aspect('equal')\n",
    "    ax2.set_facecolor('#1a1a1a')\n",
    "    \n",
    "    for i, (x, y) in enumerate(actual_microns):\n",
    "        cell_width = max(5, min(50, data.x[i, 3].item() * 20)) if data.x.shape[1] > 3 else 10\n",
    "        cell_height = cell_width * 0.8\n",
    "        \n",
    "        rect = Rectangle((x - cell_width/2, y - cell_height/2), \n",
    "                        cell_width, cell_height,\n",
    "                        facecolor='lime', edgecolor='white', \n",
    "                        alpha=0.7, linewidth=0.5)\n",
    "        ax2.add_patch(rect)\n",
    "    \n",
    "    # Draw connections\n",
    "    for i in range(0, min(500, edge_index.shape[1]), 5):\n",
    "        src, dst = edge_index[:, i]\n",
    "        ax2.plot([actual_microns[src, 0], actual_microns[dst, 0]],\n",
    "                [actual_microns[src, 1], actual_microns[dst, 1]],\n",
    "                'yellow', alpha=0.2, linewidth=0.3)\n",
    "    \n",
    "    ax2.set_title('Actual Layout (Ground Truth)', fontsize=14, color='white', pad=20)\n",
    "    ax2.set_xlabel('X Position (µm)', fontsize=12, color='white')\n",
    "    ax2.set_ylabel('Y Position (µm)', fontsize=12, color='white')\n",
    "    ax2.tick_params(colors='white')\n",
    "    ax2.grid(True, alpha=0.2, color='gray')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('industry_layout.png', dpi=dpi, facecolor='#1a1a1a')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = np.mean((predicted_microns - actual_microns) ** 2)\n",
    "    mae = np.mean(np.abs(predicted_microns - actual_microns))\n",
    "    \n",
    "    print(f\"\\nLayout Accuracy Metrics:\")\n",
    "    print(f\"   Mean Squared Error: {mse:.2f} µm²\")\n",
    "    print(f\"   Mean Absolute Error: {mae:.2f} µm\")\n",
    "    print(f\"   Average X Error: {np.mean(np.abs(predicted_microns[:, 0] - actual_microns[:, 0])):.2f} µm\")\n",
    "    print(f\"   Average Y Error: {np.mean(np.abs(predicted_microns[:, 1] - actual_microns[:, 1])):.2f} µm\")\n",
    "\n",
    "# Visualize with test data\n",
    "print(\"Creating industry-grade layout visualization...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_data = cn_test[10].to(device)\n",
    "    test_pred = model(test_data).cpu().numpy()\n",
    "\n",
    "visualize_industry_layout(test_data, test_pred)\n",
    "print(\"Industry-grade visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef786a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "339639f0",
   "metadata": {},
   "source": [
    "## Run GNN Placement Inference via GUI\n",
    "Browse and select your **standard VLSI design files** — the GNN predicts cell placement from scratch:\n",
    "- **Verilog Netlist** (`.v`) — cell instances, nets, connectivity *(required)*\n",
    "- **LEF Library** (`.lef`) — cell dimensions & pin definitions *(required)*\n",
    "- **SDC file** (`.sdc`) — timing constraints *(optional)*\n",
    "- **Floorplan DEF** (`.def`) — die area & IO pad positions only, NOT cell placement *(optional)*\n",
    "- **Model checkpoint** (`.pth`) — trained GNN weights *(required)*\n",
    "\n",
    "The cell parses these files, converts them into the 16-feature graph representation, and the **GNN predicts placement coordinates** — no existing placement required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "602c2c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching file selection GUI...\n",
      "   Select your Verilog netlist, DEF, and model .pth file.\n",
      "   LEF and SDC are optional but improve accuracy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, ttk\n",
    "import re\n",
    "import json\n",
    "import threading\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# PARSERS: Convert standard VLSI files → GNN-compatible format\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "def parse_verilog_netlist(verilog_path):\n",
    "    \"\"\"\n",
    "    Parse a Verilog netlist (.v) to extract:\n",
    "      - Cell instances (name, type)\n",
    "      - Net connections (which cells connect to which nets)\n",
    "      - Pin-level connectivity\n",
    "    \n",
    "    Supports gate-level netlists with module instantiations like:\n",
    "        NAND2X1 U123 (.A(net1), .B(net2), .Y(net3));\n",
    "    \"\"\"\n",
    "    with open(verilog_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Remove comments\n",
    "    content = re.sub(r'//.*?\\n', '\\n', content)\n",
    "    content = re.sub(r'/\\*.*?\\*/', '', content, flags=re.DOTALL)\n",
    "    \n",
    "    cells = []        # list of {name, type}\n",
    "    nets = set()       # all net names\n",
    "    pins = []          # list of {cell_idx, pin_name, net_name}\n",
    "    \n",
    "    # Pattern for module instantiation:\n",
    "    #   CellType InstanceName ( .PinName(NetName), ... );\n",
    "    # Also handles: CellType #(...) InstanceName ( ... );\n",
    "    inst_pattern = re.compile(\n",
    "        r'(\\w+)\\s+'                           # cell type\n",
    "        r'(?:#\\s*\\([^)]*\\)\\s+)?'              # optional parameters #(...)\n",
    "        r'(\\w+)\\s*\\('                          # instance name (\n",
    "        r'(.*?)\\)\\s*;',                        # pin connections );\n",
    "        re.DOTALL\n",
    "    )\n",
    "    \n",
    "    # Skip these keywords (not cell instantiations)\n",
    "    skip_keywords = {\n",
    "        'module', 'endmodule', 'input', 'output', 'inout', 'wire', 'reg',\n",
    "        'assign', 'always', 'initial', 'begin', 'end', 'if', 'else',\n",
    "        'case', 'endcase', 'for', 'while', 'function', 'endfunction',\n",
    "        'task', 'endtask', 'generate', 'endgenerate', 'parameter',\n",
    "        'localparam', 'supply0', 'supply1', 'tri', 'wand', 'wor'\n",
    "    }\n",
    "    \n",
    "    # Pin connection pattern: .PinName(NetName)\n",
    "    pin_pattern = re.compile(r'\\.(\\w+)\\s*\\(\\s*([^)]*?)\\s*\\)')\n",
    "    \n",
    "    for match in inst_pattern.finditer(content):\n",
    "        cell_type = match.group(1)\n",
    "        inst_name = match.group(2)\n",
    "        pin_section = match.group(3)\n",
    "        \n",
    "        if cell_type.lower() in skip_keywords:\n",
    "            continue\n",
    "        \n",
    "        cell_idx = len(cells)\n",
    "        cells.append({'name': inst_name, 'type': cell_type})\n",
    "        \n",
    "        # Extract pin connections\n",
    "        for pin_match in pin_pattern.finditer(pin_section):\n",
    "            pin_name = pin_match.group(1)\n",
    "            net_name = pin_match.group(2).strip()\n",
    "            \n",
    "            if net_name and net_name not in (\"\", \"1'b0\", \"1'b1\"):\n",
    "                # Handle bus notation: net[3:0] or net[2]\n",
    "                base_net = re.sub(r'\\[.*?\\]', '', net_name).strip()\n",
    "                if base_net:\n",
    "                    nets.add(base_net)\n",
    "                    pins.append({\n",
    "                        'cell_idx': cell_idx,\n",
    "                        'pin_name': pin_name,\n",
    "                        'net_name': base_net\n",
    "                    })\n",
    "    \n",
    "    if not cells:\n",
    "        raise ValueError(\"No cell instantiations found in Verilog file. \"\n",
    "                         \"Make sure it's a gate-level / structural netlist.\")\n",
    "    \n",
    "    # Build net-to-index mapping\n",
    "    net_list = sorted(nets)\n",
    "    net_to_idx = {n: i for i, n in enumerate(net_list)}\n",
    "    \n",
    "    # Build node_attr equivalent: [cell_names, cell_types]\n",
    "    cell_names = [c['name'] for c in cells]\n",
    "    cell_types = [c['type'] for c in cells]\n",
    "    node_attr = np.array([cell_names, cell_types], dtype=object)\n",
    "    \n",
    "    # Build pin_attr equivalent: [pin_names, net_indices, node_indices]\n",
    "    pin_names_arr = [p['pin_name'] for p in pins]\n",
    "    pin_nets_arr = [net_to_idx[p['net_name']] for p in pins]\n",
    "    pin_nodes_arr = [p['cell_idx'] for p in pins]\n",
    "    pin_attr = np.array([pin_names_arr, pin_nets_arr, pin_nodes_arr], dtype=object)\n",
    "    \n",
    "    print(f\"   [Verilog] Parsed {len(cells)} cells, {len(net_list)} nets, {len(pins)} pins\")\n",
    "    return node_attr, pin_attr, cell_names, cell_types\n",
    "\n",
    "\n",
    "def parse_def_file(def_path):\n",
    "    \"\"\"\n",
    "    Parse a DEF file (.def) to extract cell placement coordinates.\n",
    "    \n",
    "    Returns a dict: cell_name -> [x_min, y_min, x_max, y_max]\n",
    "    \n",
    "    Handles DEF COMPONENTS section:\n",
    "        - CELL_NAME CellType + PLACED ( X Y ) Orientation ;\n",
    "        - CELL_NAME CellType + FIXED ( X Y ) Orientation ;\n",
    "    \"\"\"\n",
    "    with open(def_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    placement = {}\n",
    "    \n",
    "    # Find COMPONENTS section\n",
    "    comp_match = re.search(r'COMPONENTS\\s+\\d+\\s*;(.*?)END\\s+COMPONENTS', content, re.DOTALL)\n",
    "    if not comp_match:\n",
    "        raise ValueError(\"No COMPONENTS section found in DEF file.\")\n",
    "    \n",
    "    comp_section = comp_match.group(1)\n",
    "    \n",
    "    # Parse each component: - INST_NAME CELL_TYPE + PLACED/FIXED ( X Y ) ORIENT ;\n",
    "    comp_pattern = re.compile(\n",
    "        r'-\\s+(\\S+)\\s+(\\S+)\\s*'                    # instance name, cell type\n",
    "        r'.*?(?:PLACED|FIXED|COVER)\\s*'              # placement status\n",
    "        r'\\(\\s*(-?\\d+)\\s+(-?\\d+)\\s*\\)\\s*'           # ( X Y )\n",
    "        r'(\\w+)\\s*;',                                # orientation\n",
    "        re.DOTALL\n",
    "    )\n",
    "    \n",
    "    for match in comp_pattern.finditer(comp_section):\n",
    "        inst_name = match.group(1)\n",
    "        x = int(match.group(3))\n",
    "        y = int(match.group(4))\n",
    "        # Assume default cell size (will be overridden by LEF if available)\n",
    "        placement[inst_name] = [x, y, x + 1, y + 1]\n",
    "    \n",
    "    if not placement:\n",
    "        raise ValueError(\"No placed components found in DEF file.\")\n",
    "    \n",
    "    print(f\"   [DEF] Parsed {len(placement)} placed components\")\n",
    "    return placement\n",
    "\n",
    "\n",
    "def parse_lef_file(lef_path):\n",
    "    \"\"\"\n",
    "    Parse a LEF file (.lef) to extract cell library dimensions.\n",
    "    \n",
    "    Returns a dict: cell_type -> {width, height, pins: [pin_name, ...]}\n",
    "    \"\"\"\n",
    "    with open(lef_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    cell_lib = {}\n",
    "    \n",
    "    # Find each MACRO definition\n",
    "    macro_pattern = re.compile(\n",
    "        r'MACRO\\s+(\\S+)\\s*\\n(.*?)END\\s+\\1',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    \n",
    "    for match in macro_pattern.finditer(content):\n",
    "        macro_name = match.group(1)\n",
    "        macro_body = match.group(2)\n",
    "        \n",
    "        # Extract SIZE width BY height\n",
    "        size_match = re.search(r'SIZE\\s+([\\d.]+)\\s+BY\\s+([\\d.]+)', macro_body)\n",
    "        width, height = 1.0, 1.0\n",
    "        if size_match:\n",
    "            width = float(size_match.group(1))\n",
    "            height = float(size_match.group(2))\n",
    "        \n",
    "        # Extract CLASS\n",
    "        class_match = re.search(r'CLASS\\s+(\\w+)', macro_body)\n",
    "        cell_class = class_match.group(1) if class_match else \"CORE\"\n",
    "        \n",
    "        # Extract PIN names\n",
    "        pin_names = re.findall(r'PIN\\s+(\\S+)', macro_body)\n",
    "        \n",
    "        cell_lib[macro_name] = {\n",
    "            'width': width,\n",
    "            'height': height,\n",
    "            'class': cell_class,\n",
    "            'pins': pin_names\n",
    "        }\n",
    "    \n",
    "    print(f\"   [LEF] Parsed {len(cell_lib)} cell definitions\")\n",
    "    return cell_lib\n",
    "\n",
    "\n",
    "def parse_sdc_file(sdc_path):\n",
    "    \"\"\"\n",
    "    Parse an SDC file (.sdc) to extract timing constraints.\n",
    "    \n",
    "    Returns a dict with:\n",
    "      - clock_period: float (ns)\n",
    "      - clock_name: str\n",
    "      - input_delays: dict\n",
    "      - output_delays: dict\n",
    "      - max_fanout: int (if set)\n",
    "    \"\"\"\n",
    "    with open(sdc_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    constraints = {\n",
    "        'clock_period': 10.0,  # default 10ns\n",
    "        'clock_name': 'clk',\n",
    "        'input_delays': {},\n",
    "        'output_delays': {},\n",
    "        'max_fanout': None,\n",
    "        'max_transition': None,\n",
    "    }\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('#') or not line:\n",
    "            continue\n",
    "        \n",
    "        # create_clock -period VALUE -name NAME\n",
    "        clk_match = re.search(r'create_clock\\s+.*?-period\\s+([\\d.]+)', line)\n",
    "        if clk_match:\n",
    "            constraints['clock_period'] = float(clk_match.group(1))\n",
    "            name_match = re.search(r'-name\\s+(\\S+)', line)\n",
    "            if name_match:\n",
    "                constraints['clock_name'] = name_match.group(1)\n",
    "        \n",
    "        # set_input_delay\n",
    "        in_delay = re.search(r'set_input_delay\\s+.*?([\\d.]+).*?\\[get_ports\\s+(\\S+)\\]', line)\n",
    "        if in_delay:\n",
    "            constraints['input_delays'][in_delay.group(2)] = float(in_delay.group(1))\n",
    "        \n",
    "        # set_output_delay\n",
    "        out_delay = re.search(r'set_output_delay\\s+.*?([\\d.]+).*?\\[get_ports\\s+(\\S+)\\]', line)\n",
    "        if out_delay:\n",
    "            constraints['output_delays'][out_delay.group(2)] = float(out_delay.group(1))\n",
    "        \n",
    "        # set_max_fanout\n",
    "        fanout_match = re.search(r'set_max_fanout\\s+([\\d.]+)', line)\n",
    "        if fanout_match:\n",
    "            constraints['max_fanout'] = int(float(fanout_match.group(1)))\n",
    "        \n",
    "        # set_max_transition\n",
    "        trans_match = re.search(r'set_max_transition\\s+([\\d.]+)', line)\n",
    "        if trans_match:\n",
    "            constraints['max_transition'] = float(trans_match.group(1))\n",
    "    \n",
    "    print(f\"   [SDC] Clock: {constraints['clock_name']} @ {constraints['clock_period']}ns, \"\n",
    "          f\"{len(constraints['input_delays'])} input delays, \"\n",
    "          f\"{len(constraints['output_delays'])} output delays\")\n",
    "    return constraints\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# CONVERTER: Parsed data → GNN-compatible PyTorch Geometric Data\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "def convert_to_gnn_data(node_attr, pin_attr, placement_dict, cell_lib=None, sdc_constraints=None):\n",
    "    \"\"\"\n",
    "    Convert parsed VLSI files into the 16-feature PyTorch Geometric Data object\n",
    "    that VLSIPlacementGNN expects.\n",
    "    \n",
    "    Args:\n",
    "        node_attr: [2, N] array — cell names and types\n",
    "        pin_attr:  [3, M] array — pin names, net indices, node indices\n",
    "        placement_dict: dict cell_name → [x_min, y_min, x_max, y_max]\n",
    "        cell_lib: (optional) LEF library — cell_type → {width, height, class, pins}\n",
    "        sdc_constraints: (optional) SDC timing constraints\n",
    "    \"\"\"\n",
    "    cell_names = list(node_attr[0])\n",
    "    cell_types = list(node_attr[1])\n",
    "    num_nodes = len(cell_names)\n",
    "    \n",
    "    # ── Coordinates & sizes from placement + LEF ──\n",
    "    coords = np.zeros((num_nodes, 2), dtype=np.float32)\n",
    "    sizes = np.zeros((num_nodes, 2), dtype=np.float32)\n",
    "    areas = np.zeros(num_nodes, dtype=np.float32)\n",
    "    \n",
    "    for i, name in enumerate(cell_names):\n",
    "        if name in placement_dict:\n",
    "            bbox = placement_dict[name]\n",
    "            coords[i, 0] = (bbox[0] + bbox[2]) / 2.0\n",
    "            coords[i, 1] = (bbox[1] + bbox[3]) / 2.0\n",
    "            w = abs(bbox[2] - bbox[0])\n",
    "            h = abs(bbox[3] - bbox[1])\n",
    "            \n",
    "            # If LEF available, use real dimensions\n",
    "            ctype = cell_types[i]\n",
    "            if cell_lib and ctype in cell_lib:\n",
    "                w = cell_lib[ctype]['width']\n",
    "                h = cell_lib[ctype]['height']\n",
    "            \n",
    "            sizes[i] = [max(w, 0.001), max(h, 0.001)]\n",
    "        else:\n",
    "            # Cell not placed — random initial position\n",
    "            coords[i] = np.random.rand(2) * 1000\n",
    "            sizes[i] = [1.0, 1.0]\n",
    "        \n",
    "        areas[i] = sizes[i, 0] * sizes[i, 1]\n",
    "    \n",
    "    # ── Normalize ──\n",
    "    coord_min = coords.min(axis=0)\n",
    "    coord_range = coords.max(axis=0) - coord_min + 1e-8\n",
    "    coords_norm = (coords - coord_min) / coord_range\n",
    "    sizes_norm = sizes / (sizes.max() + 1e-8)\n",
    "    total_chip_area = coord_range[0] * coord_range[1] + 1e-8\n",
    "    \n",
    "    # ── Cell classification ──\n",
    "    is_macro = np.zeros(num_nodes, dtype=np.float32)\n",
    "    cell_category = np.zeros(num_nodes, dtype=np.float32)\n",
    "    type_encoding = np.zeros(num_nodes, dtype=np.float32)\n",
    "    \n",
    "    unique_types = sorted(set(cell_types))\n",
    "    type_to_idx = {t: i for i, t in enumerate(unique_types)}\n",
    "    type_enc = np.array([type_to_idx[t] for t in cell_types], dtype=np.float32)\n",
    "    if len(unique_types) > 1:\n",
    "        type_encoding = type_enc / (len(unique_types) - 1)\n",
    "    \n",
    "    for i, (ct, a) in enumerate(zip(cell_types, areas)):\n",
    "        macro, filler, cat = classify_cell(ct, a)\n",
    "        is_macro[i] = float(macro)\n",
    "        cell_category[i] = cat / 3.0\n",
    "    \n",
    "    # ── Build edges from pin_attr (netlist connectivity) ──\n",
    "    edge_index, n_orphans, n_nets, cell_connectivity = build_netlist_edges(\n",
    "        pin_attr, node_attr, cell_names, star_threshold=10\n",
    "    )\n",
    "    \n",
    "    # Fallback to KNN if no netlist edges\n",
    "    if edge_index.shape[1] == 0:\n",
    "        print(\"   WARNING: No netlist edges built, falling back to KNN\")\n",
    "        k_neighbors = min(8, num_nodes - 1)\n",
    "        if num_nodes > 1:\n",
    "            tree = KDTree(coords_norm)\n",
    "            _, indices = tree.query(coords_norm, k=k_neighbors + 1)\n",
    "            src = np.repeat(np.arange(num_nodes), k_neighbors)\n",
    "            dst = indices[:, 1:].flatten()\n",
    "            edge_index = np.stack([src, dst], axis=0).astype(np.int64)\n",
    "        else:\n",
    "            edge_index = np.array([[0], [0]], dtype=np.int64)\n",
    "        for idx in range(num_nodes):\n",
    "            cell_connectivity[idx] = {'pin_count': 0, 'net_count': 0, 'avg_fanout': 0, 'max_fanout': 0}\n",
    "    \n",
    "    # ── Connectivity features ──\n",
    "    pin_counts = np.array([cell_connectivity.get(i, {}).get('pin_count', 0) for i in range(num_nodes)], dtype=np.float32)\n",
    "    net_counts = np.array([cell_connectivity.get(i, {}).get('net_count', 0) for i in range(num_nodes)], dtype=np.float32)\n",
    "    avg_fanouts = np.array([cell_connectivity.get(i, {}).get('avg_fanout', 0) for i in range(num_nodes)], dtype=np.float32)\n",
    "    max_fanouts = np.array([cell_connectivity.get(i, {}).get('max_fanout', 0) for i in range(num_nodes)], dtype=np.float32)\n",
    "    \n",
    "    pin_counts_norm = pin_counts / (pin_counts.max() + 1e-8)\n",
    "    net_counts_norm = net_counts / (net_counts.max() + 1e-8)\n",
    "    avg_fanouts_norm = avg_fanouts / (avg_fanouts.max() + 1e-8)\n",
    "    max_fanouts_norm = max_fanouts / (max_fanouts.max() + 1e-8)\n",
    "    \n",
    "    relative_area = areas / (total_chip_area + 1e-8)\n",
    "    relative_area_norm = relative_area / (relative_area.max() + 1e-8)\n",
    "    \n",
    "    aspect_ratio = sizes[:, 0] / (sizes[:, 1] + 1e-8)\n",
    "    aspect_ratio = np.clip(aspect_ratio, 0, 10)\n",
    "    aspect_ratio_norm = aspect_ratio / (aspect_ratio.max() + 1e-8)\n",
    "    \n",
    "    conn_importance = pin_counts * net_counts\n",
    "    conn_importance_norm = conn_importance / (conn_importance.max() + 1e-8)\n",
    "    \n",
    "    # Average neighbor size\n",
    "    neighbor_area_avg = np.zeros(num_nodes, dtype=np.float32)\n",
    "    if edge_index.shape[1] > 0:\n",
    "        ei = edge_index\n",
    "        for e in range(ei.shape[1]):\n",
    "            src_n, dst_n = ei[0, e], ei[1, e]\n",
    "            neighbor_area_avg[src_n] += areas[dst_n]\n",
    "        degree = np.bincount(ei[0], minlength=num_nodes).astype(np.float32)\n",
    "        degree[degree == 0] = 1\n",
    "        neighbor_area_avg /= degree\n",
    "    neighbor_area_norm = neighbor_area_avg / (neighbor_area_avg.max() + 1e-8)\n",
    "    \n",
    "    # ── Assemble 16-feature vector ──\n",
    "    node_features = np.zeros((num_nodes, 16), dtype=np.float32)\n",
    "    node_features[:, 0]  = coords_norm[:, 0]\n",
    "    node_features[:, 1]  = coords_norm[:, 1]\n",
    "    node_features[:, 2]  = sizes_norm[:, 0]\n",
    "    node_features[:, 3]  = sizes_norm[:, 1]\n",
    "    node_features[:, 4]  = np.log1p(areas)\n",
    "    node_features[:, 4] /= (node_features[:, 4].max() + 1e-8)\n",
    "    node_features[:, 5]  = type_encoding\n",
    "    node_features[:, 6]  = is_macro\n",
    "    node_features[:, 7]  = cell_category\n",
    "    node_features[:, 8]  = pin_counts_norm\n",
    "    node_features[:, 9]  = net_counts_norm\n",
    "    node_features[:, 10] = avg_fanouts_norm\n",
    "    node_features[:, 11] = max_fanouts_norm\n",
    "    node_features[:, 12] = relative_area_norm\n",
    "    node_features[:, 13] = aspect_ratio_norm\n",
    "    node_features[:, 14] = neighbor_area_norm\n",
    "    node_features[:, 15] = conn_importance_norm\n",
    "    \n",
    "    # ── Build Data object ──\n",
    "    data = Data(\n",
    "        x=torch.tensor(node_features, dtype=torch.float),\n",
    "        edge_index=torch.tensor(edge_index, dtype=torch.long),\n",
    "        y=torch.tensor(coords_norm, dtype=torch.float)  # original placement as reference\n",
    "    )\n",
    "    data.num_cells = num_nodes\n",
    "    data.sample_name = \"user_design\"\n",
    "    data.design_name = \"user_design\"\n",
    "    data.original_coords = torch.tensor(coords, dtype=torch.float)\n",
    "    data.is_macro = torch.tensor(is_macro, dtype=torch.float)\n",
    "    data.areas = torch.tensor(areas, dtype=torch.float)\n",
    "    data.cell_names = cell_names\n",
    "    data.cell_types = cell_types\n",
    "    data.coord_min = coord_min\n",
    "    data.coord_range = coord_range\n",
    "    \n",
    "    print(f\"\\n   Converted to GNN format:\")\n",
    "    print(f\"      Cells: {num_nodes:,}\")\n",
    "    print(f\"      Edges: {edge_index.shape[1]:,}\")\n",
    "    print(f\"      Macros: {int(is_macro.sum())}\")\n",
    "    print(f\"      Features: 16 per cell\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# GUI: File selection + inference + visualization\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "def run_gnn_inference_gui():\n",
    "    \"\"\"\n",
    "    Launch a tkinter GUI to:\n",
    "      1. Browse for Verilog netlist, DEF, LEF (optional), SDC (optional), Model\n",
    "      2. Parse all files automatically\n",
    "      3. Run GNN inference\n",
    "      4. Display predicted placement\n",
    "    \"\"\"\n",
    "    \n",
    "    # ── State ──\n",
    "    file_paths = {\n",
    "        'verilog': None,\n",
    "        'def': None,\n",
    "        'lef': None,\n",
    "        'sdc': None,\n",
    "        'model': None,\n",
    "    }\n",
    "    \n",
    "    # ── Root window ──\n",
    "    root = tk.Tk()\n",
    "    root.title(\"VLSI GNN Placement - File Input\")\n",
    "    root.geometry(\"720x560\")\n",
    "    root.configure(bg='#1e1e2e')\n",
    "    root.resizable(False, False)\n",
    "    \n",
    "    style = ttk.Style()\n",
    "    style.theme_use('clam')\n",
    "    style.configure('Header.TLabel', background='#1e1e2e', foreground='#cdd6f4',\n",
    "                    font=('Segoe UI', 16, 'bold'))\n",
    "    style.configure('Sub.TLabel', background='#1e1e2e', foreground='#a6adc8',\n",
    "                    font=('Segoe UI', 9))\n",
    "    style.configure('File.TLabel', background='#313244', foreground='#cdd6f4',\n",
    "                    font=('Consolas', 9), padding=5)\n",
    "    style.configure('Browse.TButton', font=('Segoe UI', 9))\n",
    "    style.configure('Run.TButton', font=('Segoe UI', 11, 'bold'))\n",
    "    style.configure('Status.TLabel', background='#1e1e2e', foreground='#a6e3a1',\n",
    "                    font=('Segoe UI', 10))\n",
    "    \n",
    "    # ── Header ──\n",
    "    ttk.Label(root, text=\"VLSI GNN Placement Inference\", style='Header.TLabel').pack(pady=(18, 2))\n",
    "    ttk.Label(root, text=\"Select your design files below. Verilog + DEF are required; LEF and SDC are optional.\",\n",
    "              style='Sub.TLabel').pack(pady=(0, 14))\n",
    "    \n",
    "    # ── File rows ──\n",
    "    file_frame = tk.Frame(root, bg='#1e1e2e')\n",
    "    file_frame.pack(fill='x', padx=30)\n",
    "    \n",
    "    path_labels = {}\n",
    "    \n",
    "    file_defs = [\n",
    "        ('verilog', 'Verilog Netlist (.v)', [('Verilog', '*.v'), ('All', '*.*')], True),\n",
    "        ('def',     'DEF Placement (.def)', [('DEF', '*.def'), ('All', '*.*')], True),\n",
    "        ('lef',     'LEF Library (.lef)',    [('LEF', '*.lef'), ('All', '*.*')], False),\n",
    "        ('sdc',     'SDC Constraints (.sdc)',[('SDC', '*.sdc'), ('All', '*.*')], False),\n",
    "        ('model',   'Model Weights (.pth)',  [('PyTorch', '*.pth'), ('All', '*.*')], True),\n",
    "    ]\n",
    "    \n",
    "    for i, (key, label_text, ftypes, required) in enumerate(file_defs):\n",
    "        row = tk.Frame(file_frame, bg='#1e1e2e')\n",
    "        row.pack(fill='x', pady=4)\n",
    "        \n",
    "        req_tag = \" *\" if required else \"\"\n",
    "        lbl = tk.Label(row, text=f\"{label_text}{req_tag}\", bg='#1e1e2e', fg='#cdd6f4',\n",
    "                       font=('Segoe UI', 10), width=28, anchor='w')\n",
    "        lbl.pack(side='left')\n",
    "        \n",
    "        path_var = tk.StringVar(value=\"No file selected\")\n",
    "        path_lbl = tk.Label(row, textvariable=path_var, bg='#313244', fg='#bac2de',\n",
    "                            font=('Consolas', 9), width=38, anchor='w', relief='flat', padx=6, pady=3)\n",
    "        path_lbl.pack(side='left', padx=(4, 6))\n",
    "        path_labels[key] = path_var\n",
    "        \n",
    "        def make_browse(k=key, ft=ftypes, pv=path_var):\n",
    "            def browse():\n",
    "                p = filedialog.askopenfilename(filetypes=ft)\n",
    "                if p:\n",
    "                    file_paths[k] = p\n",
    "                    pv.set(os.path.basename(p))\n",
    "            return browse\n",
    "        \n",
    "        btn = ttk.Button(row, text=\"Browse\", command=make_browse(), style='Browse.TButton')\n",
    "        btn.pack(side='left')\n",
    "    \n",
    "    # ── Required note ──\n",
    "    ttk.Label(file_frame, text=\"* = required\", style='Sub.TLabel').pack(anchor='w', pady=(6, 0))\n",
    "    \n",
    "    # ── Status ──\n",
    "    status_var = tk.StringVar(value=\"Ready — select files and click Run Inference\")\n",
    "    status_label = tk.Label(root, textvariable=status_var, bg='#1e1e2e', fg='#a6e3a1',\n",
    "                            font=('Segoe UI', 10), wraplength=650, justify='left')\n",
    "    status_label.pack(pady=(16, 4), padx=30, anchor='w')\n",
    "    \n",
    "    # ── Progress bar ──\n",
    "    progress = ttk.Progressbar(root, mode='determinate', length=660)\n",
    "    progress.pack(pady=(2, 12), padx=30)\n",
    "    \n",
    "    # ── Run button ──\n",
    "    def run_inference():\n",
    "        # Validate required files\n",
    "        for key in ['verilog', 'def', 'model']:\n",
    "            if not file_paths[key]:\n",
    "                nice = {'verilog': 'Verilog Netlist', 'def': 'DEF File', 'model': 'Model Weights'}[key]\n",
    "                messagebox.showerror(\"Missing File\", f\"Please select a {nice} file.\")\n",
    "                return\n",
    "        \n",
    "        status_var.set(\"Parsing files...\")\n",
    "        progress['value'] = 0\n",
    "        root.update_idletasks()\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Parse Verilog\n",
    "            status_var.set(\"Step 1/5 — Parsing Verilog netlist...\")\n",
    "            progress['value'] = 10\n",
    "            root.update_idletasks()\n",
    "            node_attr, pin_attr, cell_names, cell_types = parse_verilog_netlist(file_paths['verilog'])\n",
    "            \n",
    "            # Step 2: Parse DEF\n",
    "            status_var.set(\"Step 2/5 — Parsing DEF placement...\")\n",
    "            progress['value'] = 25\n",
    "            root.update_idletasks()\n",
    "            placement_dict = parse_def_file(file_paths['def'])\n",
    "            \n",
    "            # Step 3: Parse LEF (optional)\n",
    "            cell_lib = None\n",
    "            if file_paths['lef']:\n",
    "                status_var.set(\"Step 3/5 — Parsing LEF library...\")\n",
    "                progress['value'] = 35\n",
    "                root.update_idletasks()\n",
    "                cell_lib = parse_lef_file(file_paths['lef'])\n",
    "            \n",
    "            # Step 4: Parse SDC (optional)\n",
    "            sdc_constraints = None\n",
    "            if file_paths['sdc']:\n",
    "                status_var.set(\"Step 4/5 — Parsing SDC constraints...\")\n",
    "                progress['value'] = 40\n",
    "                root.update_idletasks()\n",
    "                sdc_constraints = parse_sdc_file(file_paths['sdc'])\n",
    "            \n",
    "            # Step 5: Convert to GNN format\n",
    "            status_var.set(\"Step 5/5 — Converting to GNN format & running inference...\")\n",
    "            progress['value'] = 50\n",
    "            root.update_idletasks()\n",
    "            \n",
    "            gnn_data = convert_to_gnn_data(\n",
    "                node_attr, pin_attr, placement_dict,\n",
    "                cell_lib=cell_lib,\n",
    "                sdc_constraints=sdc_constraints\n",
    "            )\n",
    "            \n",
    "            # Load model\n",
    "            progress['value'] = 60\n",
    "            root.update_idletasks()\n",
    "            \n",
    "            inference_model = VLSIPlacementGNN(\n",
    "                input_dim=16, hidden_dim=128, output_dim=2,\n",
    "                num_layers=4, heads=4\n",
    "            ).to(device)\n",
    "            \n",
    "            ckpt = torch.load(file_paths['model'], map_location=device)\n",
    "            inference_model.load_state_dict(ckpt['model_state_dict'])\n",
    "            inference_model.eval()\n",
    "            \n",
    "            status_var.set(\"Running GNN inference...\")\n",
    "            progress['value'] = 75\n",
    "            root.update_idletasks()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                gnn_data_device = gnn_data.to(device)\n",
    "                predictions = inference_model(gnn_data_device).cpu().numpy()\n",
    "            \n",
    "            progress['value'] = 90\n",
    "            root.update_idletasks()\n",
    "            \n",
    "            # ── Save results ──\n",
    "            output_dir = os.path.dirname(file_paths['verilog'])\n",
    "            \n",
    "            # Convert predictions back to micron coordinates\n",
    "            pred_coords = predictions * gnn_data.coord_range + gnn_data.coord_min\n",
    "            orig_coords = gnn_data.original_coords.numpy()\n",
    "            \n",
    "            # Save JSON\n",
    "            result = {\n",
    "                'design': gnn_data.design_name,\n",
    "                'num_cells': int(gnn_data.num_cells),\n",
    "                'cells': []\n",
    "            }\n",
    "            for idx in range(len(predictions)):\n",
    "                result['cells'].append({\n",
    "                    'name': gnn_data.cell_names[idx],\n",
    "                    'type': gnn_data.cell_types[idx],\n",
    "                    'predicted_x': float(pred_coords[idx, 0]),\n",
    "                    'predicted_y': float(pred_coords[idx, 1]),\n",
    "                    'original_x': float(orig_coords[idx, 0]),\n",
    "                    'original_y': float(orig_coords[idx, 1]),\n",
    "                })\n",
    "            \n",
    "            json_out = os.path.join(output_dir, \"gnn_placement_result.json\")\n",
    "            with open(json_out, 'w') as f:\n",
    "                json.dump(result, f, indent=2)\n",
    "            \n",
    "            # Save DEF\n",
    "            def_out = os.path.join(output_dir, \"gnn_placement_result.def\")\n",
    "            with open(def_out, 'w') as f:\n",
    "                f.write(f\"DESIGN {gnn_data.design_name} ;\\n\")\n",
    "                f.write(f\"COMPONENTS {gnn_data.num_cells} ;\\n\")\n",
    "                for idx in range(len(predictions)):\n",
    "                    x_int = int(pred_coords[idx, 0])\n",
    "                    y_int = int(pred_coords[idx, 1])\n",
    "                    f.write(f\"  - {gnn_data.cell_names[idx]} {gnn_data.cell_types[idx]}\"\n",
    "                            f\" + PLACED ( {x_int} {y_int} ) N ;\\n\")\n",
    "                f.write(\"END COMPONENTS\\n\")\n",
    "            \n",
    "            progress['value'] = 100\n",
    "            status_var.set(f\"Done! Results saved to:\\n  {json_out}\\n  {def_out}\")\n",
    "            root.update_idletasks()\n",
    "            \n",
    "            # ── Close GUI and visualize in matplotlib ──\n",
    "            root.destroy()\n",
    "            \n",
    "            # Visualize\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\"GNN PLACEMENT INFERENCE COMPLETE\")\n",
    "            print(\"=\" * 70)\n",
    "            print(f\"   Cells processed: {gnn_data.num_cells:,}\")\n",
    "            print(f\"   Results saved to: {output_dir}\")\n",
    "            print(f\"   JSON: {os.path.basename(json_out)}\")\n",
    "            print(f\"   DEF:  {os.path.basename(def_out)}\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            # Plot predicted vs original\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(16, 8), dpi=120)\n",
    "            fig.patch.set_facecolor('#1a1a1a')\n",
    "            \n",
    "            for ax, coords_plot, title, color in [\n",
    "                (axes[0], pred_coords, 'GNN Predicted Placement', 'cyan'),\n",
    "                (axes[1], orig_coords, 'Original Placement (from DEF)', 'lime'),\n",
    "            ]:\n",
    "                ax.set_facecolor('#1a1a1a')\n",
    "                \n",
    "                # Color macros differently\n",
    "                macro_mask = gnn_data.is_macro.numpy().astype(bool)\n",
    "                \n",
    "                ax.scatter(coords_plot[~macro_mask, 0], coords_plot[~macro_mask, 1],\n",
    "                          s=3, c=color, alpha=0.6, label='Standard cells')\n",
    "                if macro_mask.any():\n",
    "                    ax.scatter(coords_plot[macro_mask, 0], coords_plot[macro_mask, 1],\n",
    "                              s=40, c='red', marker='s', alpha=0.9, label='Macros')\n",
    "                \n",
    "                # Sample edges\n",
    "                ei = gnn_data.edge_index.numpy()\n",
    "                for e in range(0, min(500, ei.shape[1]), 5):\n",
    "                    s, d = ei[0, e], ei[1, e]\n",
    "                    ax.plot([coords_plot[s, 0], coords_plot[d, 0]],\n",
    "                            [coords_plot[s, 1], coords_plot[d, 1]],\n",
    "                            'yellow', alpha=0.1, linewidth=0.3)\n",
    "                \n",
    "                ax.set_title(title, fontsize=14, color='white', pad=12)\n",
    "                ax.set_xlabel('X (µm)', color='white')\n",
    "                ax.set_ylabel('Y (µm)', color='white')\n",
    "                ax.tick_params(colors='white')\n",
    "                ax.legend(fontsize=9, loc='upper right')\n",
    "                ax.grid(True, alpha=0.15, color='gray')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, 'gnn_placement_comparison.png'),\n",
    "                        dpi=150, facecolor='#1a1a1a', bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            # Error metrics\n",
    "            mse = np.mean((pred_coords - orig_coords) ** 2)\n",
    "            mae = np.mean(np.abs(pred_coords - orig_coords))\n",
    "            print(f\"\\nPlacement Metrics vs Original:\")\n",
    "            print(f\"   MSE:  {mse:.2f}\")\n",
    "            print(f\"   MAE:  {mae:.2f}\")\n",
    "            print(f\"   Max displacement: {np.max(np.linalg.norm(pred_coords - orig_coords, axis=1)):.2f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            status_var.set(f\"ERROR: {str(e)}\")\n",
    "            progress['value'] = 0\n",
    "            messagebox.showerror(\"Error\", f\"An error occurred:\\n\\n{str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    run_btn = tk.Button(root, text=\"▶  Run Inference\", command=run_inference,\n",
    "                        bg='#89b4fa', fg='#1e1e2e', font=('Segoe UI', 12, 'bold'),\n",
    "                        relief='flat', padx=30, pady=8, cursor='hand2',\n",
    "                        activebackground='#74c7ec', activeforeground='#1e1e2e')\n",
    "    run_btn.pack(pady=(4, 18))\n",
    "    \n",
    "    root.mainloop()\n",
    "\n",
    "\n",
    "# ── Launch the GUI ──\n",
    "print(\"Launching file selection GUI...\")\n",
    "print(\"   Select your Verilog netlist, DEF, and model .pth file.\")\n",
    "print(\"   LEF and SDC are optional but improve accuracy.\\n\")\n",
    "run_gnn_inference_gui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af987c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
